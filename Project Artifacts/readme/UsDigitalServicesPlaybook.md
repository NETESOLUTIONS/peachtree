# Digital Services Playbook Checklist
1. Play 1: Understand what people need
    * Checklist:
        - [x] Early in the project, spend time with current and prospective users of the service
        - [x] Use a range of qualitative and quantitative research methods to determine people’s goals, needs, and behaviors; be thoughtful about the time spent
        - [x] Test prototypes of solutions with real people, in the field if possible
        - [x] Document the findings about user goals, needs, behaviors, and preferences
        - [x] Share findings with the team and agency leadership
        - [x] Create a prioritized list of tasks the user is trying to accomplish, also known as "user stories"
        - [x] As the digital service is being built, regularly test it with potential users to ensure it meets people’s needs 
    * Key Questions:
        * Who are your primary users?
            * Primary users of PeachTree are Medical practioners, Hospital Administrative users and general public interested in knowing the adverse events of Food, Drugs and Devices.
        * What user needs will this service address?
            * This web service, 'peachtree,' is a place for a user to easily and efficently sort through drugs and their                 corresponding after effects with a reported count/rate.  
        * Why does the user want or need this service?
            * Peachtree will be a good first stop during the prescription decision-making process, or to crosscheck any                  unintended consequences after an intial prescription. 
        * Which people will have the most difficulty with the service?
            * We follow user-centered design principals with the hope that all user actions will be intuitive. 
        * Which research methods were used?
            * A/B, Usabilty, User Experience, Expert, Build, and Speed Testing were all used to progress from strat to                   finish. 
        * What were the key findings?
            * The key findings included the usability, ease of use and reporting the information as-is.
        * How were the findings documented? Where can future team members access the documentation?
            * Key finding were documented with comprehensive write-ups, styored as User Testing by version number and stored                 in correspnding Github repo main page. 
        * How often are you testing with real people?
            * Daily. 
2. Play 2: Address the whole experience, from start to finish
    * Checklist:
        - [x] Understand the different points at which people will interact with the service – both online and in person
        - [x] Identify pain points in the current way users interact with the service, and prioritize these according to user needs
        - [ ] Design the digital parts of the service so that they are integrated with the offline touch points people use to interact with the service
        - [ ] Develop metrics that will measure how well the service is meeting user needs at each step of the service
    * Key Questions:
        * What are the different ways (both online and offline) that people currently accomplish the task the digital service is designed to help with?
            * There is no readily available perscription drug after effects database. One would have to consult a doctor or              expert on the drug. One could also get this information from medical journals for which you must pay a premium. 
        * Where are user pain points in the current way people accomplish the task?
            * Inefficiency. To find this data you either have to know your destination or wade through a lot of bloated                  information which is unessecasry for the above defined user of this service, eg, definitions, picture, etc. 
        * Where does this specific project fit into the larger way people currently obtain the service being offered?
            * Peachtree would help medical professions bypass websites with too much information. The list with related after             effects is easier to referece and sort. 
        * What metrics will best indicate how well the service is working for its users?
            * 
3. Play 3: Make it simple and intuitive
    * Checklist:
        - [x] Create or use an existing, simple, and flexible design style guide for the service
        - [x] Use the design style guide consistently for related digital services
        - [ ] Give users clear information about where they are in each step of the process
        - [x] Follow accessibility best practices to ensure all people can use the service
        - [ ] Provide users with a way to exit and return later to complete the process
        - [x] Use language that is familiar to the user and easy to understand
        - [x] Use language and design consistently throughout the service, including online and offline touch points 
    * Key Questions:
        * What primary tasks are the user trying to accomplish?
            * Access a list of drugs, an item detail page for selected drug, and a dynamic visual of summary                             counts/statistics. 
        * Is the language as plain and universal as possible?
            * yes
        * Why does the user want or need this service?
            * The user wants this service so that he or she may efficiently access a drug database, see their documented                after effects, and then cross check with patient or project history and see if the drug is compatible. 
        * What languages is your service offered in?
            * 
        * How does the service’s design visually relate to other government services?
            * 
4. Play 4: Build the service using agile and iterative practices
    * Checklist:
        - [x] Ship a functioning “minimum viable product” (MVP) that solves a core user need as soon as possible, no longer than three months from the beginning of the project, using a “beta” or “test” period if needed
        - [x] Run usability tests frequently to see how well the service works and identify improvements that should be made
        - [x] Ensure the individuals building the service communicate closely using techniques such as launch meetings, war rooms, daily standups, and team chat tools
        - [x] Keep delivery teams small and focused; limit organizational layers that separate these teams from the business owners
        - [x] Release features and improvements multiple times each month
        - [x] Create a prioritized list of tasks the user is trying to accomplish, also known as "user stories"
        - [x] Use a source code version control system
        - [x] Give the entire project team access to the issue tracker and version control system 
        - [x] Use code reviews to ensure quality          
    * Key Questions:
        * How long did it take to ship the MVP? If it hasn't shipped yet, when will it?
            * We deployed a production release very two sprints and the MVP was available after the initial two sprints.
        * How long does it take for a production deployment?
            * Every two sprints resulted in a production deployment.
        * How many days or weeks are in each iteration/sprint?
            * It depends on the clients needs, but the usual is two to four weeks in other NETE development projects.  
        * Which version control system is being used?
            * Github. 
        * How are bugs tracked and tickets issued? What tool is used?
            * We use Github to task and track issues. The task/issue become formalized in github as soon a developer or                  product owner brings it to the task managers attention.   
        * How is the feature backlog managed? What tool is used?
            * We use Github to move issues from milestone/sprint to milestone/sprint or put it into the undated 'backlog'                milestone. The issues are stored there and are not forgotten. 
        * How often do you review and reprioritize the feature and bug backlog?
            * Depends on the work load or importance - from daily to weekly. 
        * How do you collect user feedback during development? How is that feedback used to improve the service?
            * Feedback was collected in sprint review meeting and was incorporated for future sprint planning.
        * At each stage of usability testing, which gaps were identified in addressing user needs?
            * Backend architecture, frontend human-centered design, visualization stubbles or successes, and user interface              and usability. 
5. Play 5: Structure budgets and contracts to support delivery
    * Checklist:
        - [ ] Budget includes research, discovery, and prototyping activities
        - [ ] Contract is structured to request frequent deliverables, not multi-month milestones
        - [ ] Contract is structured to hold vendors accountable to deliverables
        - [ ] Contract gives the government delivery team enough flexibility to adjust feature prioritization and delivery schedule as the project evolves
        - [ ] Contract ensures open source solutions are evaluated when technology choices are made
        - [ ] Contract specifies that software and data generated by third parties remains under our control, and can be reused and released to the public as appropriate and in accordance with the law
        - [ ] Contract allows us to use tools, services, and hosting from vendors with a variety of pricing models, including fixed fees and variable models like “pay-for-what-you-use” services
        - [ ] Contract specifies a warranty period where defects uncovered by the public are addressed by the vendor at no additional cost to the government 
        - [ ] Contract includes a transition of services period and transition-out plan          
    * Key Questions:
        * What is the scope of the project? What are the key deliverables?
            * The scope of the project is to create a POC to demonstrate our Agile capabilities to build an aplication using FDA data through APIs. The key deliverables are a working design prototype, working application and its associated documentation.
        * What are the milestones? How frequent are they?
            * In Github, our milestones line up with either sprints, delivery deadlines, or both. 
        * What are the performance metrics defined in the contract (e.g., response time, system uptime, time period to address priority issues)?
            * In absence of specific performance benchmarks, we have considered standard attributes including but not limited to Relevance, Scalability and results being returned in acceptable timeframe.
6. Play 6: Assign one leader and hold that person accountable
    * Checklist:
        - [x] A product owner has been identified
        - [x] All stakeholders agree that the product owner has the authority to assign tasks and make decisions about features and technical implementation details
        - [x] The product owner has a product management background with technical experience to assess alternatives and weigh tradeoffs
        - [x] The product owner has a work plan that includes budget estimates and identifies funding sources
        - [ ] The product owner has a strong relationship with the contracting officer
    * Key Questions:
        * Who is the product owner?
            * Person accountable for the delivery of the prototype took the role of product owner.
        * What organizational changes have been made to ensure the product owner has sufficient authority over and support for the project?
            * Assigned him authority for decision making and empowered him with a self organized scrum team.
        * What does it take for the product owner to add or remove a feature from the service?
            * Feedback from team and end users and research on similar products available in the market.
7. Play 7: Bring in experienced teams
    * Checklist:
        - [x] Member(s) of the team have experience building popular, high-traffic digital services
        - [x] Member(s) of the team have experience designing mobile and web applications
        - [x] Member(s) of the team have experience using automated testing frameworks
        - [x] Member(s) of the team have experience with modern development and operations (DevOps) techniques like continuous integration and continuous deployment
        - [ ] Member(s) of the team have experience securing digital services
        - [ ] A Federal contracting officer is on the internal team if a third party will be used for development work
        - [ ] A Federal budget officer is on the internal team or is a partner
        - [ ] The appropriate privacy, civil liberties, and/or legal advisor for the department or agency is a partner
8. Play 8: Choose a modern technology stack
    * Checklist:
        - [x] Choose software frameworks that are commonly used by private-sector companies creating similar services
        - [x] Whenever possible, ensure that software can be deployed on a variety of commodity hardware types
        - [x] Ensure that each project has clear, understandable instructions for setting up a local development environment, and that team members can be quickly added or removed from projects
        - [x] Consider open source software solutions at every layer of the stack
    * Key Questions:
        * What is your development stack and why did you choose it?
            * The deployment stack is nodejs for the backend and angularjs for client side framework. It was chosen because if its flexibility and ubiquity. Allowing developers to work in one programming language from the front to the back allows them to develop quickly and maintain style standards.
        * Which databases are you using and why did you choose them?
            * The database is a PostgreSQL database. PostgreSQL was chosen because of its open source nature and extensibility.
        * How long does it take for a new team member to start developing?
            * If the developer is familiar with modern tooling the time to first line of code is very minimal. For new developers onboarding should focus on tool stack to help them get up and running faster.
9. Play 9: Deploy in a flexible hosting environment
    * Checklist:
        - [x] Resources are provisioned on demand
        - [x] Resources scale based on real-time user demand
        - [x] Resources are provisioned through an API
        - [x] Resources are available in multiple regions
        - [ ] We only pay for resources we use
        - [x] Static assets are served through a content delivery network
        - [ ] Application is hosted on commodity hardware
    * Key Questions:
        * Where is your service hosted?
            * Heroku
        * What hardware does your service use to run?
            * It is run in Linux containers (LXCs) provided and provision by Heroku.
        * What is the demand or usage pattern for your service?
            * Usage can be scaled utilizing worker dynos, when load is high. Thresholds can be set to how many and when they are to be bootstrapped. The prototype does not have this implemented but it is available.
        * What happens to your service when it experiences a surge in traffic or load?
            * See above response.
        * How much capacity is available in your hosting environment?
            * Capacity can be scaled to very high levels. However if server load is extremely high it may be prudent to move to IaaS provider like AWS or Azure for further scaling customization.
        * How long does it take you to provision a new resource, like an application server?
            * It is done very quickly using either command line tools provided by or Heroku or by setting specifications in the Procfile
        * How have you designed your service to scale based on demand?
            * Somewhat, for the MVP this was deemed an unecessary cost, but it is using a more powerful dyno to ensure better first page load times.
        * How are you paying for your hosting infrastructure (e.g., by the minute, hourly, daily, monthly, fixed)?
            * monthly
        * Is your service hosted in multiple regions, availability zones, or data centers?
            * It can be, CDNs can be used as well to deliver static content.
        * In the event of a catastrophic disaster to a datacenter, how long will it take to have the service operational?
            * This application can be mirgrated and deployed in a few minutes. It can also be moved to other hosting services with relative ease.
        * What would be the impact of a prolonged downtime window?
            * In this case lack of access to the MVP.
        * What data redundancy do you have built into the system, and what would be the impact of a catastrophic data loss?
            * Data rendunancy is maintained through systematic backups. A catastrophic data loss would not actually render the application unusable just certain features would go down.
        * How often do you need to contact a person from your hosting provider to get resources or to fix an issue?
            * Never.
10. Play 10: Automate testing and deployments
    * Checklist:
        - [x] Create automated tests that verify all user-facing functionality
        - [x] Create unit and integration tests to verify modules and components
        - [x] Run tests automatically as part of the build process
        - [x] Perform deployments automatically with deployment scripts, continuous delivery services, or similar techniques
        - [ ] Conduct load and performance tests at regular intervals, including before public launch
    * Key Questions:
        * What percentage of the code base is covered by automated tests?
            * :------------- | :-------------
Statements  | 60.84%
Branches  | 25.93%
Functions | 53.38%
Lines | 61.09%
        * How long does it take to build, test, and deploy a typical bug fix?
            * This really depends on the type of bug.
        * How long does it take to build, test, and deploy a new feature into production?
            * Again, highly dependant on the feature request.
        * How frequently are builds created?
            * Everytime a commit to the master branch is made.
        * What test tools are used?
            * Karam, Chai, Mocha and BardJs
        * Which deployment automation or continuous integration tools are used?
            * Codeship
        * What is the estimated maximum number of concurrent users who will want to use the system?
            * 10
        * How many simultaneous users could the system handle, according to the most recent capacity test?
            * Have not conducted capacity tests on the MVP
        * How does the service perform when you exceed the expected target usage volume? Does it degrade gracefully or catastrophically?
            * Have not exceeded, but it will attempt graceful degradation.
        * What is your scaling strategy when demand increases suddenly?
            * Increase the number of dynos (scale horizontally)
11. Play 11: Manage security and privacy through reusable processes
    * N/A for our current prototype.
12. Play 12: Use data to drive decisions
    * Checklist:
        - [x] Monitor system-level resource utilization in real time
        - [x] Monitor system performance in real-time (e.g. response time, latency, throughput, and error rates)
        - [x] Ensure monitoring can measure median, 95th percentile, and 98th percentile performance
        - [x] Create automated alerts based on this monitoring
        - [ ] Track concurrent users in real-time, and monitor user behaviors in the aggregate to determine how well the service meets user needs
        - [ ] Publish metrics internally
        - [ ] Publish metrics externally
        - [ ] Use an experimentation tool that supports multivariate testing in production 
    * Key Questions:
        * What are the key metrics for the service?
            * Response Time, Throughput (request/min), Dyno Load, Memory Usage, Memory Swap.
        * How have these metrics performed over the life of the service?
            * No alarms to date.
        * Which system monitoring tools are in place?
            * Heroku's built in monitoring tools.
        * What is the targeted average response time for your service? What percent of requests take more than 1 second, 2 seconds, 4 seconds, and 8 seconds?
            * 1 second is the target, currently first page load is roughly 1.5 seconds due to some blocking css and not utilizing CDNs.
        * What is the average response time and percentile breakdown (percent of requests taking more than 1s, 2s, 4s, and 8s) for the top 10 transactions?
            * Average time to first byte is 180 ms.
        * What is the volume of each of your service’s top 10 transactions? What is the percentage of transactions started vs. completed?
            * 
        * What is your service’s monthly uptime target?
            * Heroku maintains an uptime around 99.99 (significant to two nines)
        * What is your service’s monthly uptime percentage, including scheduled maintenance? Excluding scheduled maintenance?
            * 100% so far.
        * How does your team receive automated alerts when incidents occur?
            * Emails are sent to admins.
13. Play 13: Default to open
    * N/A for prototype application

Taken from the [U.S. Digital Services Playbook](https://playbook.cio.gov/) 
